\documentclass[11pt]{article}

% Basic packages
\usepackage[margin=1in]{geometry}
\setlength{\parindent}{0pt}
\setlength{\parskip}{0.5em}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{microtype}
\usepackage{textcomp}  % Additional text symbols
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{nicefrac}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{natbib}

\title{DRIFTBENCH: Measuring Reliability Half-Life of RAG Systems Under Knowledge Drift}

\author{
  Debu Sinha \\
  Independent Researcher \\
  \texttt{debusinha2009@gmail.com}
}

\begin{document}

\maketitle

\begin{abstract}
\noindent Systems that rely on external knowledge---RAG pipelines, tool-using agents, and cached memory systems---face an unmeasured vulnerability: \emph{knowledge drift}, the divergence between indexed documentation and current ground truth. We introduce \textbf{DRIFTBENCH}, a benchmark of 77 organically-derived drift tasks from real version changes in FastAPI, Pydantic, and LangChain. Our findings overturn a common assumption: \textbf{drift effects are heterogeneous}. Average accuracy often \emph{improves} under drift (V1: 64.9\% to V2: 70.1\%), as updated documentation clarifies ambiguities. However, \textbf{Silent Failure Rate (SFR) persists at 12\%} regardless of accuracy direction, revealing safety risks invisible to aggregate metrics. We introduce complementary metrics---Reliability Half-Life and SFR---and an Oracle-Doc diagnostic showing that 13\% of failures are reasoning-caused even with gold retrieval. Our task-level analysis identifies three drift regimes: corrective, breaking, and masking. We conclude that accuracy alone is insufficient for monitoring RAG reliability under drift.
\end{abstract}

%==============================================================================
\section{Introduction}
%==============================================================================

Retrieval-Augmented Generation (RAG) has become the dominant paradigm for grounding large language models in external knowledge \citep{lewis2020retrieval, guu2020realm}. Production RAG systems now power customer support, code assistants, and enterprise search, where reliability is paramount. Yet these systems harbor a critical vulnerability: \emph{knowledge drift}.

Knowledge drift occurs when the information in a RAG system's indexed corpus diverges from ground truth. This happens continuously in practice:
\begin{itemize}
    \item \textbf{API versioning}: FastAPI 0.100 changed \texttt{orm\_mode} to \texttt{from\_attributes}
    \item \textbf{Library restructuring}: LangChain split imports across \texttt{langchain-core} and \texttt{langchain-community}
    \item \textbf{Default changes}: Pydantic v2 renamed \texttt{.dict()} to \texttt{.model\_dump()}
\end{itemize}

When drift occurs, RAG systems fail \emph{silently}---returning outdated information with high confidence, providing no signal to users that the answer may be stale. This silent failure mode is particularly dangerous because it evades standard monitoring.

Despite extensive work on RAG evaluation \citep{chen2024benchmarking, es2024ragas}, no existing benchmark treats drift as a first-class experimental variable. Prior work evaluates static snapshots; we argue that \textbf{temporal robustness} is equally critical for production deployment.

\paragraph{Contributions.} We introduce DRIFTBENCH and present findings that challenge conventional assumptions about knowledge drift:

\begin{enumerate}
    \item \textbf{DRIFTBENCH benchmark}: 77 organically-derived drift tasks from real software version changes (FastAPI 41, LangChain 26, Tool APIs 10), with paired v1/v2 corpora and evidence.

    \item \textbf{Drift is not uniformly harmful}: We show that average accuracy can \emph{improve} under drift (64.9\% $\rightarrow$ 70.1\%), overturning the assumption that stale documentation degrades performance.

    \item \textbf{Accuracy is insufficient for safety}: Silent Failure Rate persists at $\sim$12\% [5--19\% CI] regardless of accuracy direction, revealing risks invisible to aggregate metrics.

    \item \textbf{Complementary metrics}: We introduce Reliability Half-Life ($d_{1/2}$) and formalize SFR as drift-specific safety signals that accuracy alone cannot provide.

    \item \textbf{Drift taxonomy}: Task-level analysis reveals three regimes---corrective, breaking, and masking drift---with distinct safety implications for production monitoring.
\end{enumerate}

%==============================================================================
\section{Related Work}
%==============================================================================

\paragraph{RAG Benchmarks.} Existing benchmarks evaluate RAG on static corpora \citep{ragsurvey2025}. RAGAS \citep{es2024ragas} measures faithfulness and relevance but assumes indexed documents are correct---it cannot detect failures when the corpus itself is stale. RGB \citep{chen2024benchmarking} tests noise robustness (corrupted passages) but noise is random, not systematic like version drift. \textbf{RARE} \citep{rare2025} evaluates robustness over ``dynamic, time-sensitive corpora'' but focuses on query/document perturbations rather than systematic version drift. \textbf{CRUD-RAG} \citep{lyu2024crud} is closest to our work: it examines dynamic knowledge via Create/Update/Delete operations. However, CRUD-RAG uses \emph{synthetic} edits to Wikipedia, whereas DRIFTBENCH uses \emph{organic} drift from real software versioning. This distinction matters: synthetic edits may not capture the cascading, interdependent nature of real API changes (e.g., renaming \texttt{orm\_mode} also requires updating import paths).

\paragraph{Agent and Tool Benchmarks.} AgentBench \citep{liu2023agentbench} and recent agent evaluation surveys \citep{agenteval2025} evaluate tool-using agents but \emph{assume static tool schemas}. Real APIs evolve: parameters are renamed, defaults change, endpoints are deprecated. While DRIFTBENCH directly evaluates RAG systems, the framework naturally extends to tool-using agents that rely on retrieved documentation for API usage---a critical gap given that production agents call external APIs that update independently.

\paragraph{Temporal Knowledge and Knowledge Drift.} StreamingQA \citep{liska2022streamingqa} and TempLAMA \citep{dhingra2022time} study temporal reasoning about world events. Recent work on medical knowledge drift \citep{medknowledgedrift2025} highlights how LLMs provide outdated advice when clinical guidelines evolve. DRIFTBENCH addresses a related but distinct problem: not \emph{what changed in the world} or \emph{in the model's weights}, but \emph{what changed in the indexed documentation}. A model may know that Pydantic v2 exists, yet still fail when its retrieval corpus describes v1 behavior.

\paragraph{Calibration Under Distribution Shift.} ECE \citep{guo2017calibration} measures confidence calibration on i.i.d. test sets. We introduce SFR to measure calibration failure \emph{specifically under drift}---the high-stakes regime where confident errors cause real harm. Standard ECE does not capture this: a model can have low ECE on static data yet catastrophic SFR under drift. Recent work demonstrates that semantic similarity metrics fail on real hallucinations despite succeeding on synthetic benchmarks \citep{sinha2025semantic}, and that agent calibration inverts model rankings under cost-weighted evaluation \citep{sinha2025atcb}. Together, these findings establish that accuracy alone is insufficient for safe AI deployment.

This work is part of a broader investigation into \textit{reliability under distributional shift}, examining how AI systems fail silently when conditions deviate from training or indexing time.

%==============================================================================
\section{The DRIFTBENCH Benchmark}
%==============================================================================

\subsection{Task Design}

DRIFTBENCH tasks are derived from \emph{organic drift}---real breaking changes between software versions. Each task consists of:

\begin{itemize}
    \item \textbf{Question} $q$: A factoid question about library behavior
    \item \textbf{Answer v1} $y^{(1)}$: Correct answer under version 1
    \item \textbf{Answer v2} $y^{(2)}$: Correct answer under version 2
    \item \textbf{Evidence v1/v2}: Supporting documentation snippets
    \item \textbf{Drift type}: \texttt{default\_changed}, \texttt{behavior\_changed}, \texttt{param\_renamed}, \texttt{import\_changed}
\end{itemize}

\paragraph{Data Sources.} We mine breaking changes from:
\begin{itemize}
    \item \textbf{FastAPI/Pydantic} (41 tasks): Pydantic v1$\rightarrow$v2 migration, including \texttt{orm\_mode}, \texttt{@validator}, \texttt{.dict()}
    \item \textbf{LangChain} (26 tasks): Package restructuring (v0.0$\rightarrow$v0.2), LCEL adoption, \texttt{.run()}$\rightarrow$\texttt{.invoke()}
    \item \textbf{Tool APIs} (10 tasks): Parameter renames, unit changes, type constraints
\end{itemize}

\subsection{Drift Dose Protocol}

We define \textbf{drift dose} $d \in [0, 1]$ as the fraction of corpus documents updated to v2:
\begin{equation}
    \text{Corpus}(d) = \{(1-d) \cdot \text{docs}_{v1}\} \cup \{d \cdot \text{docs}_{v2}\}
\end{equation}

This allows controlled experiments: $d=0$ is pure v1 (baseline), $d=1$ is pure v2 (full drift), and intermediate values simulate partial corpus staleness.

\subsection{Metrics}

\paragraph{Success Rate.} Standard accuracy: $S(d) = P(\hat{y} = y \mid d)$

\paragraph{Reliability Half-Life.} The drift dose at which accuracy drops to half of baseline:
\begin{equation}
    d_{1/2} = \inf\{d : S(d) \leq 0.5 \cdot S(0)\}
\end{equation}
Systems with higher $d_{1/2}$ are more drift-robust.

\paragraph{Silent Failure Rate.} The probability of confident, unhedged errors:
\begin{equation}
    \text{SFR}_\tau(d) = P(\hat{y} \neq y \land c \geq \tau \land u = 0)
\end{equation}
where $c$ is model confidence, $u$ is uncertainty flag, and $\tau = 0.8$ by default.

\paragraph{Oracle Gap.} The accuracy difference between Oracle-Doc (gold retrieval) and full RAG:
\begin{equation}
    \text{Gap} = S_{\text{Oracle}}(d) -- S_{\text{RAG}}(d)
\end{equation}
A large gap indicates retrieval-dominated failures.

%==============================================================================
\section{Experiments}
%==============================================================================

\subsection{Setup}

\paragraph{Systems.} We evaluate:
\begin{itemize}
    \item \textbf{Vanilla RAG (Term Overlap)}: Top-3 retrieval with BM25-style term matching, GPT-4o-mini generation
    \item \textbf{Vanilla RAG (Dense)}: Top-3 retrieval with all-MiniLM-L6-v2 embeddings, GPT-4o-mini generation
    \item \textbf{Oracle-Doc}: Gold evidence injected directly (bypasses retrieval)
\end{itemize}

\paragraph{Protocol.} For each task, we:
\begin{enumerate}
    \item Index the appropriate corpus version
    \item Query the system with the question
    \item Extract answer, confidence, and uncertainty flag
    \item Compare against expected answer for that corpus version
\end{enumerate}

\subsection{Results}

\paragraph{V1 vs V2 Comparison.} Table~\ref{tab:v1v2} shows accuracy under baseline (v1) and drifted (v2) corpora.

\begin{table}[h]
\centering
\caption{Accuracy and Silent Failure Rate (SFR: confident errors with no uncertainty expression) under version drift. N=77 tasks, GPT-4o-mini, 95\% bootstrap CI in brackets.}
\label{tab:v1v2}
\begin{tabular}{lccccc}
\toprule
System & V1 Acc. & V2 Acc. & $\Delta$ & V1 SFR & V2 SFR \\
\midrule
Term Overlap & 64.9\% [53--74] & 70.1\% [60--79] & +5.2\% & 11.7\% & 11.7\% \\
Dense (MiniLM) & 80.5\% [71--88] & 85.7\% [77--94] & +5.2\% & 14.3\% & 10.4\% \\
Oracle-Doc & 80.5\% [71--88] & 87.0\% [79--94] & +6.5\% & 15.6\% & 7.8\% \\
\bottomrule
\end{tabular}
\end{table}


Figure~\ref{fig:v1v2} illustrates these results.

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{fig_v1v2.pdf}
\caption{Left: Accuracy comparison between V1 (baseline) and V2 (drifted) corpora. All systems show improved accuracy under drift. Right: Silent Failure Rate persists at 10--16\% regardless of accuracy gains.}
\label{fig:v1v2}
\end{figure}

\textbf{Key finding}: Contrary to intuition, accuracy \emph{improves} under drift across all systems (+5--6\%). This suggests that V2 documentation often clarifies ambiguities present in V1. However, \textbf{SFR persists at 10--16\%} regardless of accuracy direction---silent failures remain a constant risk. Dense retrieval outperforms term overlap (80.5\% vs 64.9\%), indicating that semantic embeddings (MiniLM on technical documentation) handle documentation updates well. The Oracle gap (87\% vs 70--86\%) reveals that 13\% of failures are reasoning-caused even with gold retrieval.

\paragraph{Drift Dose Protocol.} DRIFTBENCH supports intermediate drift doses $d \in [0,1]$ where $d$ is the fraction of corpus updated to V2. Our main results compare endpoints ($d=0$ vs $d=1$). The protocol enables future work on dose-response curves and threshold identification.

\paragraph{Silent Failure Rate Analysis.} With default prompting, SFR persists at $\sim$12\% across all conditions---a consistent safety risk that accuracy improvements do not eliminate. This is our central finding: \textbf{accuracy and SFR are decoupled under drift}.

We additionally conduct a \emph{deployment stress test}: forcing confident answers via a ``no-hedging'' prompt simulates production systems that suppress uncertainty for UX reasons. Under this stress test, \textbf{SFR reaches 90\%}, revealing that hedging behavior is the primary defense against silent failures. Production systems optimizing for confident responses face extreme risk when knowledge drifts.

\subsection{Analysis}

\paragraph{Why does accuracy improve under drift?} V2 documentation often resolves ambiguities present in V1: clearer parameter descriptions, explicit default values, and fixed inconsistencies. This suggests that ``fresh'' documentation is not just different but often \emph{better}.

\paragraph{Why does SFR persist?} Silent failures arise from confident hallucination when retrieval returns plausible-but-wrong documents. Accuracy improvements do not eliminate this risk because different tasks fail silently in V1 vs V2---the \emph{population} of silent failures shifts, but their \emph{rate} remains stable.

\paragraph{Oracle gap interpretation.} Oracle-Doc achieves 87\% (vs 70--86\% for RAG), indicating that 13\% of failures are reasoning-caused even with perfect retrieval. This is lower than prior work suggested, but non-negligible.

%==============================================================================
\section{Limitations}
%==============================================================================

DRIFTBENCH currently covers three Python libraries; broader coverage (JavaScript, Rust, cloud APIs) would strengthen generalization claims. Our 77 tasks enable statistical analysis with bootstrap confidence intervals, but larger scale would improve statistical power. We evaluate one generator model (GPT-4o-mini); additional model families would strengthen generalization. The benchmark focuses on factoid questions; multi-hop reasoning and code generation tasks remain future work.

%==============================================================================
\section{Broader Impact}
%==============================================================================

DRIFTBENCH highlights a critical failure mode in production RAG systems that current evaluation practices miss. We hope this benchmark drives development of drift-robust architectures, including version-aware retrieval, staleness detection, and confidence calibration under distribution shift. The SFR metric specifically targets safety-critical deployments where silent failures can cause real harm.

%==============================================================================
\section{Conclusion}
%==============================================================================

We introduced DRIFTBENCH, a benchmark for evaluating RAG systems under knowledge drift, and demonstrated that drift effects are heterogeneous: accuracy can \emph{improve} while Silent Failure Rate persists. This finding overturns the assumption that stale documentation uniformly degrades performance and reveals that accuracy alone is insufficient for monitoring RAG reliability. The Reliability Half-Life and SFR metrics provide complementary safety signals that accuracy cannot capture. We release DRIFTBENCH to catalyze research on drift-robust AI systems.

\bibliographystyle{plainnat}
\bibliography{references}

\end{document}
